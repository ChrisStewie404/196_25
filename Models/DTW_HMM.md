# C题

## 2401298 

1. **表现指标（问题1）：** 重加权策略，（AUC）方法  
2. **动量存在性（问题2）：** 假设检验（Ljung-Box Q检验和Runs检验）
3. **动量预测（问题3）：** 随机森林回归，贝叶斯网络模型，信息熵

### **Ljung-Box Q 检验**  判断时间序列的自相关关系
**定义**：  
Ljung-Box Q 检验是一种用于检测时间序列数据中**自相关性显著性**的方法。它能够评估数据是否表现出随机性，尤其适用于时间序列的残差分析。  

**公式**：  
Ljung-Box 统计量的计算公式为：  

$$
Q = n(n+2) \sum_{k=1}^h \frac{\hat{\rho}_k^2}{n-k}
$$

其中：  
- $Q$：Ljung-Box 统计量，近似服从 $\chi^2$ 分布；  
- $n$：样本大小；  
- $h$：考虑的自相关滞后数；  
- $\hat{\rho}_k$：在滞后 $k$ 时的样本自相关系数。  

**检验假设**：  
- **原假设 (H₀)**：序列无显著自相关性，符合白噪声假设；  
- **备择假设 (H₁)**：序列存在显著自相关性。  

**结果解释**：  
- 若 $p$ 值小于显著性水平（如 0.05），则拒绝 $H₀$，表示数据存在显著的自相关性；  
- 若 $p$ 值大于显著性水平，则无法拒绝 $H₀$，数据可能符合随机性假设。  

---

### **游程检验（Runs Test）**  判断时间序列的自相关关系
**定义**：  
游程检验是一种非参数统计方法，用于检测序列中的元素是否呈现随机性。游程定义为一系列连续相同元素的序列。例如，在序列“+++--++”中，存在 3 个游程。  

**统计量计算**：  
游程检验统计量 $Z$ 的计算公式为：  

$$
Z = \frac{R - E[R]}{\sqrt{\text{Var}(R)}}
$$

其中：  
- $R$：实际游程数；  
- $E[R]$：游程数的期望值，计算公式为：  
  $$
  E[R] = 1 + \frac{2n_1n_2}{n_1 + n_2}
  $$  
- $\text{Var}(R)$：游程数的方差，计算公式为：  
  $$
  \text{Var}(R) = \frac{2n_1n_2(2n_1n_2 - n_1 - n_2)}{(n_1+n_2)^2(n_1+n_2-1)}
  $$  
- $n_1$ 和 $n_2$：序列中两类元素（如“+”和“-”）的数量。  

**检验假设**：  
- **原假设 (H₀)**：序列中的元素是随机分布的；  
- **备择假设 (H₁)**：序列中的元素存在非随机模式。  

**结果解释**：  
- 若 $p$ 值小于显著性水平（如 0.05），则拒绝 $H₀$，表明序列可能存在非随机模式；  
- 若 $p$ 值大于显著性水平，则无法拒绝 $H₀$，表明序列可能是随机的。  

---

### **随机森林方法**    用于回归与分类
**定义**：随机森林通过构建多个决策树，然后将它们的预测结果结合起来，得到一个比单个弱学习器更强的预测结果。
随机森林通过构建多棵决策树，并将各树的预测值进行平均（在回归问题中）或投票（在分类问题中），最终获得综合的结果。    

**Bootstrap抽样**：得到的训练集随机且彼此间独立。   

**特征重要性评估**：随着训练的进行，随机森林可以提供**特征重要性评估**，模型可以帮助我们理解哪些特征在预测过程中起到了决定性的作用。

---

### 时序贝叶斯网络模型（Temporal Bayesian Network, TBN）    离散型数据网络分析

时序贝叶斯网络（TBN）是贝叶斯网络的扩展，专门用于表示和推理带有时间依赖性的动态系统。通过引入时间切片（time slices）来建模时间序列数据，TBN能够捕捉时间序列中随机变量之间的依赖关系。

#### 核心概念

1. **时间切片（Time Slice）**：
   每个时间切片表示系统在某一时刻的状态及其相互依赖关系。时间切片中的随机变量通过贝叶斯网络表示条件依赖关系。

2. **依赖关系**：
   不同时间切片之间通过有向边连接，表示时间序列中的依赖关系。每个时间切片的节点可以依赖于前一时刻（或更早时刻）的节点。

3. **条件独立性**：
   给定时间切片的父节点，当前时间切片的节点与其他非后代时间切片的节点条件独立。

4. **贝叶斯网络结构**：
   每个时间切片本质上是一个贝叶斯网络，其中节点表示随机变量，边表示变量之间的条件依赖关系。

#### 建模步骤

1. **定义时间切片**：
   为每个时间切片定义系统的状态变量（隐变量）和观测变量。每个时间切片包含一组随机变量，通常代表某个时刻的系统状态。

2. **确定节点的条件依赖关系**：
   设定随机变量之间的依赖关系。例如，$X_t$ 可能依赖于 $X_{t-1}$ 和 $Y_{t-1}$，而 $Y_t$ 可能依赖于 $X_t$。

3. **指定条件概率分布**：
   每个节点的条件概率分布通过贝叶斯网络中的边进行描述。例如，$P(X_t | X_{t-1}, Y_{t-1})$ 表示时间$t$的隐状态$X_t$条件于前一时刻的隐状态$X_{t-1}$和观测值$Y_{t-1}$。

4. **时间切片间的依赖**：
   确定不同时间切片之间的依赖关系。例如，$X_t$ 和 $Y_t$ 可能分别依赖于 $X_{t-1}$ 和 $Y_{t-1}$，这表示当前时刻的状态依赖于前一时刻的状态。

#### 示例：天气预测模型

假设我们要建模一个简单的天气预测系统，其中每个时间步的状态是“晴天”或“雨天”，观测值为“气温”（$T_t$）。我们定义以下变量：

- $S_t$: 当前时刻的天气状态（晴天或雨天）
- $T_t$: 当前时刻的气温（观测值）
- $S_{t-1}$: 前一时刻的天气状态

##### 1. 定义时间切片
每个时间切片包括$S_t$和$T_t$。假设天气状态是一个隐变量，气温是观测变量。

##### 2. 确定条件依赖关系
- 当前时刻的天气状态$S_t$依赖于前一时刻的天气状态$S_{t-1}$。
- 当前时刻的气温$T_t$依赖于当前的天气状态$S_t$。

##### 3. 条件概率分布
- $P(S_t | S_{t-1})$: 当前天气状态条件于前一时刻的天气状态。
- $P(T_t | S_t)$: 当前气温条件于当前的天气状态。

例如，$P(S_t | S_{t-1})$可以表示为：
$$
P(S_t = \text{晴天} | S_{t-1} = \text{晴天}) = 0.8, \quad P(S_t = \text{雨天} | S_{t-1} = \text{晴天}) = 0.2
$$

同样，$P(T_t | S_t)$可以表示为：
$$
P(T_t = 30^\circ C | S_t = \text{晴天}) = 0.9, \quad P(T_t = 15^\circ C | S_t = \text{雨天}) = 0.7
$$

##### 4. 时间切片间的依赖
- $S_t$依赖于$S_{t-1}$。
- $T_t$依赖于$S_t$。

#### 总结

时序贝叶斯网络（TBN）是一种强大的模型，用于表示时间序列中的随机变量及其依赖关系。通过时间切片的结构，TBN能够捕捉时间序列中的动态依赖，适用于复杂的动态系统建模。




---

## 2401445  
1. **表现指标（问题1）：** 梯度提升树量化球员表现
2. **动量存在性（问题2）：** 反证法加比较  
3. **动量预测（问题3）：** 随机森林分类和隐马尔可夫模型（HMM）


### **梯度提升树**   回归分析


梯度提升树（GBT）是一种强大的机器学习算法，通过将多个弱学习器（通常是决策树）结合成一个强学习器来提高预测性能。

#### 1. 基本概念

梯度提升树的核心思想是：通过组合一系列简单的决策树，来构建一个强大的模型。在每一步迭代中，GBT 会构建一棵新的决策树，用来纠正之前模型的错误。具体地，每次新树的训练都是基于上一轮树的残差（误差）。

#### 2. 工作原理

梯度提升树通过以下步骤进行训练：

- **初始化模型：** 首先，从一个简单的常数模型开始（通常是目标变量的均值）。
- **计算残差：** 对于当前模型的每一轮，计算该模型的预测与真实值之间的误差（残差）。
- **拟合新树：** 使用残差作为目标值，训练一棵新树。该树试图捕捉前一轮模型预测的错误。
- **更新模型：** 将新训练的树加入到现有模型中，更新模型的预测结果。
- **迭代优化：** 重复以上过程，直到达到预定的树的数量或误差达到足够低的水平。

#### 3. 数学公式

- **基础模型：** GBT 是一种逐步加法模型，最终的预测是各棵树预测结果的加和。假设我们有一个数据集 $ X = \{x_1, x_2, ..., x_n\} $，以及对应的标签 $ y = \{y_1, y_2, ..., y_n\} $，那么 GBT 模型的预测结果可以表示为：

  $$
  F(x) = \sum_{m=1}^{M} T_m(x; \Theta_m)
  $$

  其中，$ T_m(x; \Theta_m) $ 是第 m 棵树的预测结果，$ \Theta_m $ 是树的参数，M 是树的数量。

- **模型更新：** 在第 m 步迭代中，模型的更新是基于前一步的残差来进行的，具体为：

  $$
  F_m(x) = F_{m-1}(x) + \alpha T_m(x; \Theta_m)
  $$

  其中，$ \alpha $ 是学习率，控制每棵树的贡献大小。

#### 4. 损失函数

GBT 使用损失函数来量化模型预测的误差。对于回归任务，常用的损失函数是均方误差（MSE）；对于分类任务，则通常使用对数损失函数（log loss）。在每次更新模型时，GBT 会最小化损失函数，优化模型参数。

对于二分类问题，常用的损失函数为指数损失函数：

$$
L(y,y') = \exp(-yy')
$$

#### 5. 优缺点

- **优点：**
  - **高效性：** GBT 通常能提供非常高的预测准确度，尤其在复杂任务中表现出色。
  - **灵活性：** 可以处理各种类型的数据，包括数值型和类别型特征。
  - **鲁棒性：** 对于异常值和噪声数据，GBT 具有较好的鲁棒性。
  
- **缺点：**
  - **训练时间：** 由于模型是通过迭代训练的，训练过程可能较慢，尤其是数据量较大时。
  - **容易过拟合：** 如果树的数量过多或每棵树的深度过大，模型可能会过拟合训练数据。

---
### 动态时间规整（Dynamic Time Warping, DTW）算法     判断两个时间序列的相似性

动态时间规整（DTW）算法是一种用于衡量两条时间序列相似性的算法.
- 能够处理长度不同、速度不同的序列。
- 适用于非线性对齐，使得两条序列能够在时间轴上有灵活的匹配。

#### DTW算法的基本原理
DTW的核心思想是通过最小化总距离，找到一条“对齐路径”，使得两条时间序列在时间轴上的差异尽可能小。这条路径允许在时间维度上进行“扭曲”，即序列中的某些点可以与另一个序列中的多个点对齐，或者一个序列中的多个点与另一个序列中的一个点对齐。

#### 算法步骤

1. **构建距离矩阵**  
   假设有两条时间序列：A = [a1, a2, ..., an] 和 B = [b1, b2, ..., bm]，其中n和m分别是这两条序列的长度。首先，构建一个n × m的矩阵D，其中每个元素D[i, j]表示序列A的第i个元素和序列B的第j个元素之间的距离（通常使用欧氏距离或曼哈顿距离等）。

2. **计算累计距离**  
   通过递归的方式计算累计距离，公式如下：
   $$
   D[i,j] = distance(a_i, b_j) + min{D[i-1,j], D[i,j-1], D[i-1,j-1]}
   $$
   
    其中，`distance(a_i, b_j)` 是序列A和序列B的第i和第j个元素之间的距离，`D[i-1, j]`、`D[i, j-1]` 和 `D[i-1, j-1]` 是前一个计算步骤的结果，表示了不同方向上的累计距离。通过这种递归计算，最终矩阵的右下角元素 `D[n, m]` 包含了两条序列之间的最小累计距离。

3. **寻找最优对齐路径**  
计算完距离矩阵后，可以通过回溯寻找从 `D[n, m]` 到 `D[1, 1]` 的最优对齐路径，这条路径表示了两条序列之间最优的匹配方式。

4. **评估相似性**  
最终，DTW的结果可以通过以下方式评估：
- **总DTW距离**：最优路径上的距离和，较低的总距离表示两条序列的相似度较高。
- **DTW对齐路径**：最优路径的具体轨迹，观察它与对角线的接近程度，接近对角线表明两条序列在时间上高度匹配。
- 
#### 获取评估量

1. 获取总DTW距离
总DTW距离就是累计距离矩阵中的右下角元素 \( D[n,m] \)。它表示两条时间序列经过最优对齐后，最小的累计距离。

$$
\text{总DTW距离} = D[n,m]
$$

这个值越小，表示两条序列的相似性越高。

2. 回溯获得DTW对齐路径
通过回溯可以获得最优对齐路径。回溯从距离矩阵的右下角 ` D[n,m] ` 开始，选择最小的前一个累计距离元素（上方、左方或左上方），直到回溯到 ` D[1,1] `。
   1. 从 ` D[n,m] ` 开始，选择最小的前驱（上方、左方或左上方）。
   2. 将选择的路径上的 ` (i,j) ` 位置记录下来，直到达到 `D[1,1] `。

---

### 隐马尔可夫模型 HMM     离散型数据分析

隐马尔可夫模型（Hidden Markov Model, HMM）是一种描述隐状态随机过程的统计模型，广泛应用于序列建模任务，如语音识别、自然语言处理和生物信息学。HMM 是基于马尔可夫过程的一种扩展，其中状态是隐含的，但可以通过观测值推断。

---

#### 模型组成

一个隐马尔可夫模型由五个元素组成：

1. **状态集 $S$**  
   $S = \{s_1, s_2, \dots, s_N\}$：系统可能的 $N$ 个隐状态。

2. **观测符号集 $V$**  
   $V = \{v_1, v_2, \dots, v_M\}$：可能的观测值，共 $M$ 个。

3. **状态转移概率矩阵 $A$**  
   $A = \{a_{ij}\}$，表示从状态 $s_i$ 转移到状态 $s_j$ 的概率：  
   $a_{ij} = P(q_{t+1} = s_j \mid q_t = s_i), \quad 1 \leq i, j \leq N$

4. **观测概率矩阵 $B$**  
   $B = \{b_j(k)\}$，表示在状态 $s_j$ 下观测到符号 $v_k$ 的概率：  
   $b_j(k) = P(o_t = v_k \mid q_t = s_j), \quad 1 \leq j \leq N, 1 \leq k \leq M$

5. **初始状态分布 $\pi$**  
   $\pi = \{\pi_i\}$，表示初始时刻状态为 $s_i$ 的概率：  
   $\pi_i = P(q_1 = s_i), \quad 1 \leq i \leq N$

---

#### 隐马尔可夫模型的建模步骤

##### 1. 计算问题
**问题**：给定观测序列 $O = \{o_1, o_2, \dots, o_T\}$ 和模型参数 $\lambda$，计算 $P(O \mid \lambda)$。  
**解决方法**：使用前向-后向算法。

###### 前向算法
定义前向变量：  
$\alpha_t(i) = P(o_1, o_2, \dots, o_t, q_t = s_i \mid \lambda)$  
递推公式：  
$\alpha_{t+1}(j) = \left( \sum_{i=1}^N \alpha_t(i) a_{ij} \right) b_j(o_{t+1})$  
最终结果：  
$P(O \mid \lambda) = \sum_{i=1}^N \alpha_T(i)$

---

##### 2. 解码问题
**问题**：给定观测序列 $O$ 和模型参数 $\lambda$，找到最可能的状态序列 $Q^* = \{q_1, q_2, \dots, q_T\}$。  
**解决方法**：使用维特比算法。

###### 维特比算法
定义路径概率：  
$\delta_t(i) = \max_{q_1, q_2, \dots, q_{t-1}} P(q_1, q_2, \dots, q_{t-1}, q_t = s_i, o_1, o_2, \dots, o_t \mid \lambda)$  
递推公式：  
$\delta_{t+1}(j) = \max_{i=1}^N [\delta_t(i) a_{ij}] b_j(o_{t+1})$  
最终路径通过回溯确定。

---

##### 3. 学习问题
**问题**：给定观测序列 $O$，估计模型参数 $\lambda$ 使得 $P(O \mid \lambda)$ 最大。  
**解决方法**：使用 Baum-Welch 算法（EM 算法的特例）。

###### Baum-Welch 算法步骤
1. **初始化**：随机初始化 $A, B, \pi$。
2. **E步**：计算每个状态的期望计数（使用前向和后向变量）。
3. **M步**：更新参数：  
   $a_{ij} = \frac{\sum_{t=1}^{T-1} \xi_t(i, j)}{\sum_{t=1}^{T-1} \gamma_t(i)}$  
   $b_j(k) = \frac{\sum_{t \text{ where } o_t = v_k} \gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}$  
   其中 $\xi_t(i, j)$ 和 $\gamma_t(i)$ 是辅助变量。
4. **迭代**：重复 E步和 M步，直到参数收敛。
